{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41af9b82-8c91-40dc-9cde-2603aba40280",
   "metadata": {},
   "source": [
    "## QUESTION 23: Idea 1\n",
    "Use Graph Convolutional Networks. What hyperparameters do you choose to get the optimal performance? How many layers did you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38fa9d48-f0ad-4197-8682-3c0cc2ad17b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lim213/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7960\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Load the dataset (Cora)\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "# GCN Model Definition\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        # self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # x = F.relu(x)\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # Third  GCN Layer\n",
    "        # x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = GCN(dataset.num_node_features, 16, dataset.num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Move data to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "# Training the model\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluating the model\n",
    "model.eval()\n",
    "preds = model(data.x, data.edge_index).argmax(dim=1)\n",
    "correct = (preds[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "accuracy = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c142a-a392-4aa4-8ade-19263e0475eb",
   "metadata": {},
   "source": [
    "The choice of hyperparameters we choose was 16 for the hidden_channels and used 2 layer GCM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f9fef-82b9-45b9-92c5-d19d4c328e5d",
   "metadata": {},
   "source": [
    "## QUESTION 24: Idea 2\n",
    "Extract structure-based node features using Node2Vec. Briefly describe how Node2Vec finds node features. Choose your desired classifier (one of SVM, Neural Networks, or Random Forest) and classify the documents using only Node2Vec (graph strcuture) features. Now classify the documents using only the 1433-dimensional text features. Which one outperforms? Why do you think this is the case? Combine the Node2Vec and text features and train your classifier on the combined features. What is the ebst classification accuracy you get (in terms of the percentage of test documents correctly classified)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9cda165-ce10-48e4-bbdc-4c2623cfdbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff90a2e6-38bf-4812-b187-743f3c3a60a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4abd5ef18a40e38d4995d8ff86292d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 50/50 [00:07<00:00,  6.75it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 50/50 [00:07<00:00,  6.77it/s]\n",
      "Generating walks (CPU: 3): 100%|██████████| 50/50 [00:07<00:00,  6.83it/s]\n",
      "Generating walks (CPU: 4): 100%|██████████| 50/50 [00:07<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8524\n"
     ]
    }
   ],
   "source": [
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert PyG graph to NetworkX graph\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(data.num_nodes))\n",
    "edge_list = data.edge_index.t().tolist()\n",
    "G.add_edges_from(edge_list)\n",
    "\n",
    "# Node2Vec model setup\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "\n",
    "# Train Node2Vec model\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "# Get embeddings for all nodes\n",
    "embeddings = np.array([model.wv[str(i)] for i in range(data.num_nodes)])\n",
    "\n",
    "# Labels for nodes\n",
    "labels = data.y.numpy()\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Support Vector Machine classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on test dataset\n",
    "predictions = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45eee494-8d43-46f3-96b2-e12941e4dd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM using only text features: 0.7196\n"
     ]
    }
   ],
   "source": [
    "features = data.x\n",
    "labels = data.y.numpy()\n",
    "\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert torch tensor to numpy for sklearn compatibility\n",
    "X_train = X_train.numpy()\n",
    "X_test = X_test.numpy()\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "\n",
    "# Train the SVM\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on test dataset\n",
    "predictions = svm.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy of SVM using only text features: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bbb9b0-0a49-4d44-8b2e-d069085114f6",
   "metadata": {},
   "source": [
    "node2vec outperforms compared to using only text features as node2vec is using sturctural information rather than the textual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfa0a37b-a5ee-4fb2-bb70-ac4240221c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Features Accuracy: 0.8469\n"
     ]
    }
   ],
   "source": [
    "combined_features = np.concatenate((embeddings, features.numpy()), axis=1)\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "\n",
    "# Train the SVM\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on test dataset\n",
    "predictions = svm.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Combined Features Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da700d-5327-4d71-94e3-bdf228f81abf",
   "metadata": {},
   "source": [
    "The combined feature is the one that performs the best with 84.5% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b39d53-e4bb-4046-91de-a881a88ca3ed",
   "metadata": {},
   "source": [
    "## QUESTION 25: Idea 3\n",
    "We can find the personalized PageRank of each document in seven different runs, one per class. \n",
    "In each run, select one of the classes and take the 20 seed documents of that class. Then,\r\n",
    "perform a random walk with the following customized properties: (a) teleportation takes t e\r\n",
    "random walker to one of the seed documents of that class (with a uniform probability of 1/20\r\n",
    "per seed document). Vary the teleportation probability in {0, 0.1, 0.2}. (b) the probabilit  of\r\n",
    "transitioning to neighbors is not uniform among the neighbors. Rather, it is proportional t  the\r\n",
    "cosine similarity between the text features of the current node and the next neighboring    \n",
    "Repeat part b for every teleportation probability in part a.\r\n",
    "Run the PageRank only on the GCC. for each seed node, do 1000 random walks. Maintai \r\n",
    "a class-wise visited frequency count for every unlabeled node. The predicted class for that\r\n",
    "unlabeled node is the class which lead to maximum visits to that node. Report accuracy  nd\r\n",
    "f1 sco  ass A.x2, x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8631be57-aa7b-486c-8cd7-57e893e91c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming data.x contains the feature vectors for each document/node\n",
    "# and data.edge_index contains the graph structure\n",
    "feature_vectors = data.x\n",
    "graph_edges = data.edge_index\n",
    "\n",
    "# Helper function to calculate cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    cos_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    return cos_sim\n",
    "\n",
    "# Perform the random walk for a given set of seed nodes\n",
    "def random_walk(seed_nodes, graph_edges, feature_vectors, teleportation_prob):\n",
    "    visited_counts = np.zeros(feature_vectors.shape[0], dtype=int)\n",
    "    for seed_node in seed_nodes:\n",
    "        current_node = seed_node\n",
    "        for _ in range(1000):  # Perform 1000 random walks\n",
    "            if np.random.rand() < teleportation_prob:\n",
    "                current_node = np.random.choice(seed_nodes)  # Teleport to a random seed node\n",
    "            else:\n",
    "                neighbors = graph_edges[1][graph_edges[0] == current_node]\n",
    "                if len(neighbors) > 0:\n",
    "                    # Calculate transition probabilities based on cosine similarity\n",
    "                    transition_probs = np.exp([cosine_similarity(feature_vectors[current_node], feature_vectors[neighbor]) for neighbor in neighbors])\n",
    "                    # Normalize probabilities\n",
    "                    transition_probs /= np.sum(transition_probs)\n",
    "                    # Choose the next node based on transition probabilities\n",
    "                    current_node = np.random.choice(neighbors, p=transition_probs)\n",
    "            visited_counts[current_node] += 1  # Increment visit count for the current node\n",
    "    return visited_counts\n",
    "\n",
    "# Prepare the data and labels\n",
    "train_nodes = data.train_mask == True\n",
    "labels = np.unique(data.y.numpy())\n",
    "\n",
    "# Container for visit counts of each class\n",
    "all_class_visits = []\n",
    "\n",
    "# Vary the teleportation probability\n",
    "teleportation_probs = [0, 0.1, 0.2]\n",
    "\n",
    "for label in labels:\n",
    "    # Filter nodes that are both training nodes and have the current label\n",
    "    selected_nodes_mask = train_nodes & (data.y == label)\n",
    "    selected_node_indices = selected_nodes_mask.nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # Select 20 seed nodes for the label\n",
    "    if len(selected_node_indices) >= 20:\n",
    "        seed_nodes = np.random.choice(selected_node_indices.cpu().numpy(), size=20, replace=False)\n",
    "    else:\n",
    "        # If there aren't enough nodes, take all of them as seed nodes\n",
    "        seed_nodes = selected_node_indices.cpu().numpy()\n",
    "    \n",
    "    # For each teleportation probability\n",
    "    for tp in teleportation_probs:\n",
    "        # Perform the random walk\n",
    "        visits = random_walk(seed_nodes, graph_edges, feature_vectors, tp)\n",
    "        all_class_visits.append((label, tp, visits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b92a6a2-cd5c-4d3d-bfef-2d58652eb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(tele_prob, all_class_visits):\n",
    "    data = []\n",
    "    for list in all_class_visits:\n",
    "        if (list[1] == tele_prob):\n",
    "            data.append((list[0], list[2]))\n",
    "\n",
    "    length_of_array = len(data[0][1])\n",
    "    max_indices = []\n",
    "    for i in range(length_of_array):\n",
    "        max_value = -float('inf')\n",
    "        candidates = []\n",
    "        for index, array in data:\n",
    "            if array[i] > max_value:\n",
    "                max_value = array[i]\n",
    "                candidates = [index]\n",
    "            elif array[i] == max_value:\n",
    "                candidates.append(index)\n",
    "        if candidates:\n",
    "            max_indices.append(random.choice(candidates))\n",
    "    return max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5adfc516-1c9a-4617-8e2a-cea9d0fcc641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For teleportation probability: 0\n",
      " the accuracy is 0.28 and the f1 score is 0.28\n",
      "For teleportation probability: 0.1\n",
      " the accuracy is 0.68 and the f1 score is 0.68\n",
      "For teleportation probability: 0.2\n",
      " the accuracy is 0.66 and the f1 score is 0.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import random\n",
    "for prob in ([0, 0.1, 0.2]):\n",
    "    pred = predictions(prob, all_class_visits)\n",
    "    accuracy = accuracy_score(pred, data.y.numpy())\n",
    "    f1 = f1_score(pred, data.y.numpy(), average='micro')\n",
    "    print(f\"For teleportation probability: {prob}\\n the accuracy is {accuracy:.2f} and the f1 score is {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e277e-3cd4-4063-8919-7896d3003207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
